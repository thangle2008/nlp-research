{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yelp review data\n",
    "yelp_datapath = '../dataset/yelp/yelp_academic_dataset_review.json'\n",
    "\n",
    "LINE_LIMIT = 10000\n",
    "dataset = []\n",
    "\n",
    "with open(yelp_datapath, 'r') as f:\n",
    "    num_line = 0\n",
    "    for line in f:\n",
    "        num_line = num_line + 1\n",
    "        dataset.append(json.loads(line))\n",
    "        if num_line == LINE_LIMIT:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo / Fort Apache. The chef there can make a MUCH better NY style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria in a casino. I dont care what you say... \n",
      "\n",
      "stars = 2\n"
     ]
    }
   ],
   "source": [
    "# some examples\n",
    "print(dataset[0]['text'], \"\\n\")\n",
    "print(\"stars =\", dataset[0]['stars'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using traditional ML models (Naive Bayes, SVM, ...) with Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 2000\n",
      "Counter({5: 3661, 4: 1559, 1: 1359, 3: 733, 2: 688}) Counter({5: 915, 4: 390, 1: 340, 3: 183, 2: 172})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# convert into trainable data\n",
    "raw_data = [(d['text'], d['stars']) for d in dataset]\n",
    "\n",
    "# # tokenize documents and remove stop words\n",
    "# def text_processing(d):\n",
    "#     text = map(lambda x: x.lower(), word_tokenize(d[0]))\n",
    "#     text = filter(lambda x: x not in stop_words, text)\n",
    "#     text = ' '.join(text)\n",
    "#     return (text, d[1])\n",
    "    \n",
    "# start = time.time()\n",
    "# pool = multiprocessing.Pool(processes=4)\n",
    "# raw_data = pool.map(text_processing, raw_data)\n",
    "# pool.close()\n",
    "# print(\"Time for tokenizing = \", (time.time() - start))\n",
    "\n",
    "documents, labels = zip(*raw_data)\n",
    "\n",
    "# stratified shuffle split 80-20\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(sss.split(documents, labels))\n",
    "\n",
    "X_train = [documents[i] for i in train_index]\n",
    "y_train = [labels[i] for i in train_index]\n",
    "\n",
    "X_test = [documents[i] for i in test_index]\n",
    "y_test = [labels[i] for i in test_index]\n",
    "\n",
    "# sanity check\n",
    "print(len(X_train), len(X_test))\n",
    "print(Counter(y_train), Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scores:\n",
      "0.6795\n",
      "0.51975\n",
      "test scores:\n",
      "0.5935\n",
      "0.4895\n"
     ]
    }
   ],
   "source": [
    "# build a data pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss = 'hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=1000, tol=1e-3)),\n",
    "])\n",
    "\n",
    "naive_bayes_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words=stop_words)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# fit the data\n",
    "svm_clf.fit(X_train, y_train)\n",
    "naive_bayes_clf.fit(X_train, y_train)\n",
    "\n",
    "# train scores\n",
    "print(\"train scores:\")\n",
    "print(svm_clf.score(X_train, y_train))\n",
    "print(naive_bayes_clf.score(X_train, y_train))\n",
    "\n",
    "# test scores\n",
    "print(\"test scores:\")\n",
    "print(svm_clf.score(X_test, y_test))\n",
    "print(naive_bayes_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[245   4   1   1  89]\n",
      " [ 67  12   4   8  81]\n",
      " [ 28   8   7  24 116]\n",
      " [ 17   3   2  31 337]\n",
      " [ 10   0   3  10 892]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# try to look at confusion matrix\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
